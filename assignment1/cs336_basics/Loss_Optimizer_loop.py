import torch
from torch import nn
import einops
from cs336_basics.Dot_Product_Attention import softmax

def cross_entropy(input:torch.Tensor, target):
    c = input.max(dim = -1,keepdim = True).values
    test = input -c
    idx = torch.arange(input.shape[0])
    out = torch.log(torch.exp(test).sum(dim = 1))
    return (out[idx] - test[idx,target]).mean()

from collections.abc import Callable,Iterable
from typing import Optional
import torch
import math

class SGD(torch.optim.Optimizer):
    def __init__(self,params,lr = 1e-3):
        if lr<0:
            raise ValueError(f"Invalid learning rate {lr}")
        defaults = {"lr":lr}
        super().__init__(params,defaults)
    def step(self,closure:Optional[Callable] = None):
        loss = None if closure is None else closure()
        for group in self.param_groups:
            lr = group["lr"]
            for p in group["params"]:
                if p.grad == None:
                    continue
                state = self.state[p]
                t = state.get("t",0)
                grad = p.grad.data
                p.data -= lr/math.sqrt(t+1)*grad
                state["t"] = t+1
        return loss

weights = torch.nn.Parameter(5*torch.randn(10,10))
opt = SGD([weights],lr = 1e3)

for t in range(10):
    opt.zero_grad()
    loss = (weights**2).mean()
    print(loss.cpu().item())
    loss.backward()
    opt.step()



